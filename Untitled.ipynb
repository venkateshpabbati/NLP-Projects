{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11bd4e4d-3165-4368-8743-2166dce3c2be",
   "metadata": {},
   "source": [
    "To solve an NLP problem using the AmazonReview.csv dataset, the steps below outline a methodical approach. Assume the dataset contains reviews and corresponding sentiment labels (positive, negative, etc.)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cfafdd1-7960-46eb-8acc-47d9e0767d3b",
   "metadata": {},
   "source": [
    "1. Define the Problem\n",
    "Task: Perform Sentiment Analysis using the AmazonReview dataset, predicting the sentiment (e.g., positive, negative) based on customer reviews."
   ]
  },
  {
   "cell_type": "raw",
   "id": "f174fccf-47e8-4622-8a61-2b8c0effa623",
   "metadata": {},
   "source": [
    "2. Data Understanding\n",
    "Dataset: AmazonReview.csv\n",
    "Columns: Likely includes:\n",
    "Review: The text of the review.\n",
    "Sentiment: Target variable (positive/negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43d4957c-c88c-495e-98e0-ec5b1e9eae01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Sentiment\n",
      "0  Fast shipping but this product is very cheaply...          1\n",
      "1  This case takes so long to ship and it's not e...          1\n",
      "2  Good for not droids. Not good for iPhones. You...          1\n",
      "3  The cable was not compatible between my macboo...          1\n",
      "4  The case is nice but did not have a glow light...          1\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 25000 entries, 0 to 24999\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Review     24999 non-null  object\n",
      " 1   Sentiment  25000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 390.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset and inspect its structure:\n",
    "import pandas as pd\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('AmazonReview.csv')\n",
    "\n",
    "# Check data\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15f686a0-cbcd-49ac-ac1b-6951280e4fb5",
   "metadata": {},
   "source": [
    "3. Text Preprocessing\n",
    "Preprocessing is crucial for NLP tasks. Typical steps include:\n",
    "\n",
    "a. Tokenization\n",
    "Break down the text into individual words or tokens using libraries like nltk or spaCy.\n",
    "\n",
    "b. Stop Words Removal\n",
    "Remove common words that don’t contribute much to the meaning.\n",
    "\n",
    "c. Stemming/Lemmatization\n",
    "Normalize words to their root form (e.g., “running” → “run”).\n",
    "\n",
    "d. Lowercasing\n",
    "Convert text to lowercase for uniformity.\n",
    "\n",
    "e. Punctuation and Special Characters Removal\n",
    "Clean unnecessary characters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2b65c61-663b-4a8a-8bba-317008e8f611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\venka\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  Sentiment  \\\n",
      "0  Fast shipping but this product is very cheaply...          1   \n",
      "1  This case takes so long to ship and it's not e...          1   \n",
      "2  Good for not droids. Not good for iPhones. You...          1   \n",
      "3  The cable was not compatible between my macboo...          1   \n",
      "4  The case is nice but did not have a glow light...          1   \n",
      "\n",
      "                                      cleaned_review  \n",
      "0  fast shipping product cheaply made brought gra...  \n",
      "1         case take long ship 's even worth dont buy  \n",
      "2  good droids good iphones use feature watch iph...  \n",
      "3  cable compatible macbook iphone also connector...  \n",
      "4  case nice glow light 'm disappointed product n...  \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Handle cases where text is NaN (float)\n",
    "    if isinstance(text, float):\n",
    "        return \"\"\n",
    "        \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    \n",
    "    # Remove stopwords and punctuation\n",
    "    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
    "    \n",
    "    # Lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Fill missing values in the Review column with empty strings\n",
    "df['Review'] = df['Review'].fillna(\"\")\n",
    "\n",
    "# Apply preprocessing to the review text\n",
    "df['cleaned_review'] = df['Review'].apply(preprocess_text)\n",
    "\n",
    "# Check the result\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b58ecf4a-e941-4963-9950-d14d0fda6971",
   "metadata": {},
   "source": [
    "4. Feature Extraction\n",
    "TF-IDF (Term Frequency-Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "475b5cb0-c4e4-4210-8fc8-f0c0e72c8de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the text data into numerical vectors.\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "\n",
    "# Fit and transform the data\n",
    "X = tfidf.fit_transform(df['cleaned_review']).toarray()\n",
    "\n",
    "# Target variable\n",
    "y = df['Sentiment']  # Assuming Sentiment column has labels\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "009821be-ab9f-42cd-b323-faf941ef42a4",
   "metadata": {},
   "source": [
    "5. Model Selection\n",
    "You can start with classic machine learning models or leverage advanced deep learning techniques.\n",
    "\n",
    "a. Traditional Machine Learning Models\n",
    "Logistic Regression\n",
    "Naive Bayes\n",
    "Support Vector Machine (SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fccb5bd-b82e-4a2b-bbeb-497f53f2d61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\venka\\Courses\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.56      0.62      0.59      1021\n",
      "           2       0.39      0.37      0.38      1000\n",
      "           3       0.38      0.36      0.37       985\n",
      "           4       0.42      0.39      0.40       989\n",
      "           5       0.57      0.62      0.59      1005\n",
      "\n",
      "    accuracy                           0.47      5000\n",
      "   macro avg       0.46      0.47      0.47      5000\n",
      "weighted avg       0.46      0.47      0.47      5000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example using Logistic Regression:\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d06a8d-fb23-4bfb-8f8f-33d5b6bcf769",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
